{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3216105d",
   "metadata": {},
   "source": [
    "# DS 4400 Final Project : Haiku Generator\n",
    "\n",
    "#### Ben Tunney, Glen Damian Lim\n",
    "\n",
    "#### Datasets : https://www.kaggle.com/datasets/hjhalani30/haiku-dataset (English haikus)\n",
    "\n",
    "#### Word Embeddings: GloVe from https://nlp.stanford.edu/projects/glove/ (choose Wikipedia 2014 + Gigaword 5)\n",
    "\n",
    "#### NLP models: N-gram Language Model, Recurrent Neural Network, Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f45b1ea0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-20T00:48:25.843513Z",
     "iopub.status.busy": "2023-04-20T00:48:25.843077Z",
     "iopub.status.idle": "2023-04-20T00:48:33.859589Z",
     "shell.execute_reply": "2023-04-20T00:48:33.858410Z",
     "shell.execute_reply.started": "2023-04-20T00:48:25.843473Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-19 20:59:30.033282: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "# NLP libraries\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, cmudict\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# Neural Networks libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, LSTM, Embedding, Dropout\n",
    "from keras.models import Sequential\n",
    "\n",
    "# Outside Files\n",
    "# import ngram_model as ngm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e9739c",
   "metadata": {},
   "source": [
    "### Getting data and text pre-processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9675665b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-20T00:48:33.862416Z",
     "iopub.status.busy": "2023-04-20T00:48:33.861714Z",
     "iopub.status.idle": "2023-04-20T00:48:36.222470Z",
     "shell.execute_reply": "2023-04-20T00:48:36.221195Z",
     "shell.execute_reply.started": "2023-04-20T00:48:33.862383Z"
    }
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mcmudict\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('cmudict')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/cmudict\u001b[0m\n\n  Searched in:\n    - '/Users/bentunney/nltk_data'\n    - '/opt/anaconda3/nltk_data'\n    - '/opt/anaconda3/share/nltk_data'\n    - '/opt/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/nltk/corpus/util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 84\u001b[0m     root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mzip_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mcmudict\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('cmudict')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/cmudict.zip/cmudict/\u001b[0m\n\n  Searched in:\n    - '/Users/bentunney/nltk_data'\n    - '/opt/anaconda3/nltk_data'\n    - '/opt/anaconda3/share/nltk_data'\n    - '/opt/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 35>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# create an instance of the CMUDict\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m syllable \u001b[38;5;241m=\u001b[39m \u001b[43mcmudict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdict\u001b[49m()\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mestimate_syllables\u001b[39m(word):\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/nltk/corpus/util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/nltk/corpus/util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m             root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[0;32m---> 86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[1;32m     89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/nltk/corpus/util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m         root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mcmudict\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('cmudict')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/cmudict\u001b[0m\n\n  Searched in:\n    - '/Users/bentunney/nltk_data'\n    - '/opt/anaconda3/nltk_data'\n    - '/opt/anaconda3/share/nltk_data'\n    - '/opt/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Read file and get data\n",
    "def get_haiku_data(fname):\n",
    "    df = pd.read_csv(fname)\n",
    "    sentences = df['0'] + ' ' + df['1'] + ' ' + df['2'] + ' ' \n",
    "    data = [str(sentence).split() for sentence in sentences]\n",
    "    return data\n",
    "\n",
    "# lemmatizer\n",
    "lm = WordNetLemmatizer()\n",
    "\n",
    "def contains_special(word):\n",
    "    for char in word:\n",
    "        if char.isnumeric() or (not char.isalnum()):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# process tokens\n",
    "def process_tokens(toks):\n",
    "    toks = [lm.lemmatize(word.lower()) for word in toks \n",
    "          # make sure no strings that contain only numeric characters \n",
    "          if not contains_special(word)]\n",
    "    return toks\n",
    "\n",
    "def read_haikus(data, ngram):\n",
    "    result = []\n",
    "    for sentences in data:\n",
    "        toks = nltk.word_tokenize(' '.join([word for word in sentences]))\n",
    "        processed = process_tokens(toks)\n",
    "        if len(processed) != 0 and len(processed) < 17:\n",
    "            processed = ['<h>'] * (ngram-1) + processed + ['</h>'] * (ngram-1)\n",
    "            result.append(processed)\n",
    "    return result\n",
    "\n",
    "# create an instance of the CMUDict\n",
    "syllable = cmudict.dict()\n",
    "def estimate_syllables(word):\n",
    "    try:\n",
    "        count = [len(list(y for y in x if y[-1].isdigit())) for x in syllable[word.lower()]]\n",
    "        return count\n",
    "    except KeyError:\n",
    "        return 100\n",
    "    \n",
    "!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71a09ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-20T00:48:36.225614Z",
     "iopub.status.busy": "2023-04-20T00:48:36.225179Z",
     "iopub.status.idle": "2023-04-20T00:48:37.359152Z",
     "shell.execute_reply": "2023-04-20T00:48:37.357870Z",
     "shell.execute_reply.started": "2023-04-20T00:48:36.225567Z"
    }
   },
   "outputs": [],
   "source": [
    "data = get_haiku_data('/kaggle/input/haiku-dataset/all_haiku.csv')\n",
    "# Get haikus data with trigram\n",
    "# haikus = read_haikus(data, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2737acbd",
   "metadata": {},
   "source": [
    "### Training word embeddings using Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f825966f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-20T00:49:49.457345Z",
     "iopub.status.busy": "2023-04-20T00:49:49.456844Z",
     "iopub.status.idle": "2023-04-20T00:49:49.588146Z",
     "shell.execute_reply": "2023-04-20T00:49:49.587128Z",
     "shell.execute_reply.started": "2023-04-20T00:49:49.457301Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "embedding_size = 200\n",
    "\n",
    "def train_embeddings(data):\n",
    "    return Word2Vec(sentences=haikus, vector_size=embedding_size, window=5, min_count=1, \n",
    "                 sg=1)\n",
    "    \n",
    "\n",
    "# # Train the Word2Vec model from Gensim. \n",
    "# word2vec_model = train_embeddings(haikus)\n",
    "# vocab_size = len(word2vec_model.wv.index_to_key)\n",
    "# print('Vocab size {}'.format(vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8653e190",
   "metadata": {},
   "source": [
    "# N-gram Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed21c868",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-19T23:47:43.878955Z",
     "iopub.status.busy": "2023-04-19T23:47:43.878245Z",
     "iopub.status.idle": "2023-04-19T23:47:43.893655Z",
     "shell.execute_reply": "2023-04-19T23:47:43.892637Z",
     "shell.execute_reply.started": "2023-04-19T23:47:43.878914Z"
    }
   },
   "outputs": [],
   "source": [
    "# Find haikus that are similar\n",
    "def find_similar_haikus(haikus, inputs, embeddings):\n",
    "    \"\"\"Find haikus that contain words from the given inputs\n",
    "    Parameters:\n",
    "      haikus (list): list of list of processed haikus tokens\n",
    "      inputs (list): list of words to match\n",
    "      embeddings (Word2Vec): trained word embeddings\n",
    "\n",
    "    Returns:\n",
    "      list: list of list of processed haikus tokens that contain words from the given inputs\n",
    "    \"\"\"\n",
    "    similar_words = []\n",
    "    for word in inputs:\n",
    "        # Find top 5 similar words to current word\n",
    "        find_similar = [similar_words.append(w) for w,s in embeddings.wv.most_similar(word, topn=5)]\n",
    "    training_haikus = []\n",
    "    for haiku in haikus:\n",
    "        if any(word in haiku for word in similar_words):\n",
    "            training_haikus.append(haiku)\n",
    "    return [\" \".join(haiku) for haiku in training_haikus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cdbb926",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-19T23:47:43.899838Z",
     "iopub.status.busy": "2023-04-19T23:47:43.898677Z",
     "iopub.status.idle": "2023-04-19T23:47:44.944133Z",
     "shell.execute_reply": "2023-04-19T23:47:44.942523Z",
     "shell.execute_reply.started": "2023-04-19T23:47:43.899763Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'haikus' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m similar_haikus \u001b[38;5;241m=\u001b[39m find_similar_haikus(\u001b[43mhaikus\u001b[49m, [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbasketball\u001b[39m\u001b[38;5;124m'\u001b[39m], word2vec_model)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Define new N-gram Language Model object\u001b[39;00m\n\u001b[1;32m      4\u001b[0m ngram_lm \u001b[38;5;241m=\u001b[39m ngm\u001b[38;5;241m.\u001b[39mLanguageModel(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m, line_begin\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mh\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>\u001b[39m\u001b[38;5;124m\"\u001b[39m, line_end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m</\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mh\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'haikus' is not defined"
     ]
    }
   ],
   "source": [
    "similar_haikus = find_similar_haikus(haikus, ['basketball'], word2vec_model)\n",
    "\n",
    "# Define new N-gram Language Model object\n",
    "ngram_lm = ngm.LanguageModel(3, True, line_begin=\"<\" + \"h\" + \">\", line_end=\"</\" + \"h\" + \">\")\n",
    "# Training the model with haikus similar to inputs\n",
    "ngram_lm.train(similar_haikus)\n",
    "\n",
    "for haiku in ngram_lm.generate_haiku(5):\n",
    "    for line in haiku:\n",
    "        print(line)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10fe213",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks (LSTMs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05c3b664",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-20T00:48:55.485678Z",
     "iopub.status.busy": "2023-04-20T00:48:55.485098Z",
     "iopub.status.idle": "2023-04-20T00:48:55.498852Z",
     "shell.execute_reply": "2023-04-20T00:48:55.496372Z",
     "shell.execute_reply.started": "2023-04-20T00:48:55.485642Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_embeddings(model, tokenizer):\n",
    "    '''Loads and parses embeddings trained in earlier.\n",
    "    Parameters and return values are up to you.\n",
    "    '''\n",
    "    vocab = list(model.wv.index_to_key)\n",
    "    word_to_index = tokenizer.word_index\n",
    "\n",
    "    word_to_embedding = {}\n",
    "    index_to_embedding = {}\n",
    "\n",
    "    for word in vocab:\n",
    "        embedding = model.wv[word]\n",
    "        word_to_embedding[word] = embedding\n",
    "        index_to_embedding[word_to_index[word]] = embedding\n",
    "    return word_to_embedding, index_to_embedding\n",
    "\n",
    "# Produced pre-padded data for LSTM network\n",
    "def padded_data(encoded, seq_length):\n",
    "    X = []\n",
    "    y = []\n",
    "    for row in encoded:\n",
    "        for i in range(1, len(row) - 1):\n",
    "            X.append(row[:i])\n",
    "            y.append(row[i])\n",
    "    X = pad_sequences(X, maxlen = seq_length - 1)\n",
    "    return X, y\n",
    "\n",
    "def data_generator(X: list, y: list, num_sequences_per_batch: int, vocab_size: int, index_to_embedding: dict) -> (list,list):\n",
    "    '''\n",
    "    Returns data generator to be used by feed_forward\n",
    "    https://wiki.python.org/moin/Generators\n",
    "    https://realpython.com/introduction-to-python-generators/\n",
    "    \n",
    "    Yields batches of embeddings and labels to go with them.\n",
    "    Use one hot vectors to encode the labels \n",
    "    (see the to_categorical function)\n",
    "\n",
    "    '''\n",
    "    # inputs\n",
    "    i = 0\n",
    "    while i < len(X):\n",
    "        end_index = i + num_sequences_per_batch\n",
    "        # if we ran out of data\n",
    "        if end_index >= len(X) - 1:\n",
    "            i = 0\n",
    "            end_index = i + num_sequences_per_batch\n",
    "        \n",
    "        inputs = [val for val in X[i:end_index]]\n",
    "        # outputs into one hot encoding\n",
    "        outputs = [to_categorical(val, vocab_size, dtype = 'int32') for val in y[i:end_index]]\n",
    "        yield np.array(inputs), np.array(outputs)\n",
    "        i += num_sequences_per_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64befffe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-20T00:50:04.286964Z",
     "iopub.status.busy": "2023-04-20T00:50:04.285882Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "tokenizer = Tokenizer()\n",
    "haikus = read_haikus(data, 1)\n",
    "# Using 50% of training data due to limited RAM\n",
    "haikus = haikus[:math.floor(len(haikus) * 0.50)]\n",
    "word2vec_model = train_embeddings(haikus)\n",
    "vocab_size = len(word2vec_model.wv.index_to_key)\n",
    "tokenizer.fit_on_texts(haikus)\n",
    "\n",
    "# Embeddings\n",
    "word_to_embedding, index_to_embedding = read_embeddings(word2vec_model, tokenizer)\n",
    "# Embedding for zero index\n",
    "index_to_embedding[0] = np.zeros((embedding_size,))\n",
    "word_to_embedding[''] = np.zeros((embedding_size,))\n",
    "vocab_size = len(word_to_embedding.keys())\n",
    "\n",
    "# Encode words into index\n",
    "encoded = tokenizer.texts_to_sequences(haikus)\n",
    "seq_length = 10\n",
    "# Padded data along with sliding window\n",
    "X_encoded, y = padded_data(encoded, seq_length)\n",
    "\n",
    "# Convert X into 3D (num_instances, sequence length, embedding_size)\n",
    "X = np.zeros((len(X_encoded), seq_length - 1, embedding_size))\n",
    "for i in range(X_encoded.shape[0]):\n",
    "    for j in range(X_encoded.shape[1]):\n",
    "        word = X_encoded[i,j]\n",
    "        X[i, j, :] = index_to_embedding[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56243168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training the model\n",
    "\n",
    "# hyperparameters\n",
    "num_epochs = 10\n",
    "num_sequences_per_batch = 128\n",
    "steps_per_epoch = len(encoded)//num_sequences_per_batch\n",
    "\n",
    "# Data generator\n",
    "train_generator = data_generator(X,y, num_sequences_per_batch, vocab_size, index_to_embedding)\n",
    "\n",
    "model = Sequential()\n",
    "# LSTM layer\n",
    "model.add(LSTM(512, input_shape=(seq_length - 1, embedding_size),return_sequences=True))\n",
    "# Dropout layer to prevent overfitting\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256, input_shape=(seq_length - 1, embedding_size),return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256, input_shape=(seq_length - 1, embedding_size),return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(128, input_shape=(seq_length - 1, embedding_size),return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "model.fit(x= train_generator,\n",
    "          steps_per_epoch=steps_per_epoch,\n",
    "          epochs=num_epochs, verbose = 1)\n",
    "\n",
    "print(model.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4caf1d",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-20T00:49:26.448085Z",
     "iopub.status.idle": "2023-04-20T00:49:26.448600Z",
     "shell.execute_reply": "2023-04-20T00:49:26.448356Z",
     "shell.execute_reply.started": "2023-04-20T00:49:26.448330Z"
    }
   },
   "outputs": [],
   "source": [
    "# generate a sequence from the model\n",
    "def generate_seq(model: Sequential, \n",
    "                 tokenizer: Tokenizer, \n",
    "                 seed: list, \n",
    "                 syllable_limit: int):\n",
    "    '''\n",
    "    Parameters:\n",
    "        model: your neural network\n",
    "        tokenizer: the keras preprocessing tokenizer\n",
    "        seed: [w1, w2, w(n-1)]\n",
    "        n_words: generate a sentence of length n syllable\n",
    "    Returns: string sentence\n",
    "    '''\n",
    "    sentence = seed\n",
    "    i = 0\n",
    "    count_syllables = 0\n",
    "    while count_syllables != syllable_limit:\n",
    "        # n-1 tokens in sentence\n",
    "        curr_tokens = sentence\n",
    "        # encode our tokens\n",
    "        sequence = tokenizer.texts_to_sequences([curr_tokens])[0]\n",
    "        # pre-padding our tokens\n",
    "        sequence = np.array(pad_sequences([sequence], maxlen = seq_length-1, padding='pre'))\n",
    "        # Convert into 3D\n",
    "        embeddings = np.zeros((sequence.shape[0], sequence.shape[1], embedding_size))\n",
    "        for i in range(sequence.shape[0]):\n",
    "            for j in range(sequence.shape[1]):\n",
    "                word = sequence[i,j]\n",
    "                embeddings[i, j, :] = index_to_embedding[word]\n",
    "        # get probability distribution\n",
    "        probs = model.predict(embeddings)[0][2:]\n",
    "        # normalize probabilities and get index\n",
    "        random_choice = np.random.choice(len(probs),p = probs / np.sum(probs))\n",
    "        if random_choice != 0:\n",
    "            next_word = tokenizer.index_word[random_choice + 3]\n",
    "            # Count new syllables\n",
    "            new_count = syllables.estimate(next_word) + count_syllables\n",
    "            if next_word not in ['<h>','</h>'] and (new_count <= syllable_limit):\n",
    "                sentence.append(next_word)\n",
    "                count_syllables = new_count\n",
    "        else:\n",
    "            sentence = seed\n",
    "            count_syllables = 0\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af48ba05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-19T23:55:05.905304Z",
     "iopub.status.busy": "2023-04-19T23:55:05.904335Z",
     "iopub.status.idle": "2023-04-19T23:55:06.759221Z",
     "shell.execute_reply": "2023-04-19T23:55:06.757687Z",
     "shell.execute_reply.started": "2023-04-19T23:55:05.905252Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 762ms/step\n",
      "216\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'syllables' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_23/1304974889.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'i'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_23/2871748013.py\u001b[0m in \u001b[0;36mgenerate_seq\u001b[0;34m(model, tokenizer, seed, syllable_limit, NGRAM)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mnext_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrandom_choice\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# Count new syllables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mnew_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msyllables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_word\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcount_syllables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnext_word\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'<h>'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'</h>'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnew_count\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0msyllable_limit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'syllables' is not defined"
     ]
    }
   ],
   "source": [
    "seed = # USER QUERY HERE as a list of one string (EX: ['wind'])\n",
    "def generate_haiku(seed):\n",
    "    \"\"\"Generates n haikus from a trained language model\n",
    "    Parameters:\n",
    "      n (int): the number of haikus to generate\n",
    "\n",
    "    Returns:\n",
    "      list: a list containing strings, one per generated sentence\n",
    "    \"\"\"\n",
    "    haiku = []\n",
    "    line_1 = generate_seq(model, tokenizer, seed, 5).split(self.line_begin)[-1]\n",
    "    line_2 = generate_seq(model, tokenizer, line_1[-1], 5).split(self.line_begin)[-1]\n",
    "    line_3 = generate_seq(model, tokenizer, line_2[-1], 5).split(self.line_begin)[-1]\n",
    "    haiku.append(line_1)\n",
    "    haiku.append(line_2)\n",
    "    haiku.append(line_3)\n",
    "    haikus.append(haiku)\n",
    "    return haiku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b2d552",
   "metadata": {},
   "outputs": [],
   "source": [
    "for haiku in generate_haiku(5):\n",
    "    for line in haiku:\n",
    "        print(line)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c50393",
   "metadata": {},
   "source": [
    "# Plotly Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5352c0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries for ai image\n",
    "import json\n",
    "import requests\n",
    "from PIL import Image\n",
    "import io\n",
    "import re\n",
    "\n",
    "# libraries for ner\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c45799",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_TOKEN = \"\"  # token in case you want to use private API\n",
    "headers = {\n",
    "    # \"Authorization\": f\"Bearer {API_TOKEN}\",\n",
    "    \"X-Wait-For-Model\": \"true\",\n",
    "    \"X-Use-Cache\": \"false\"\n",
    "}\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/runwayml/stable-diffusion-v1-5\"\n",
    "NER = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def query(payload):\n",
    "    data = json.dumps(payload)\n",
    "    response = requests.request(\"POST\", API_URL, headers=headers, data=data)\n",
    "    return Image.open(io.BytesIO(response.content))\n",
    "\n",
    "\n",
    "def slugify(text):\n",
    "    # remove non-word characters and foreign characters\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \"-\", text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50aa4f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image(prompt, mdl):\n",
    "\n",
    "    # https://www.analyticsvidhya.com/blog/2021/06/nlp-application-named-entity-recognition-ner-in-python-with-spacy/\n",
    "    text1 = NER(prompt)\n",
    "\n",
    "    # try to get NERs\n",
    "    ners = \"\"\n",
    "    for word in text1.ents:\n",
    "        ner = word.text\n",
    "        ners += ner \n",
    "        ners += \" \"\n",
    "        \n",
    "    # if there are NERs, use for prompt\n",
    "    if ners != \"\":\n",
    "        prompt = ners\n",
    "        \n",
    "    # used stopword-removed haiku as prompt\n",
    "    else:\n",
    "        tokenized = prompt.split(\" \")\n",
    "        wordsList = [w for w in tokenized if w not in stop_words]\n",
    "        prompt = \" \".join(wordsList)\n",
    "\n",
    "    # save img\n",
    "    image = query({\"inputs\": prompt})\n",
    "    image.save(f\"image_{mdl}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f419a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_haikus(word):\n",
    "    \n",
    "    similar_haikus = find_similar_haikus(haikus, [word], word2vec_model)\n",
    "\n",
    "    # Define new N-gram Language Model object\n",
    "    ngram_lm = ngm.LanguageModel(2, True, line_begin=\"<\" + \"h\" + \">\", line_end=\"</\" + \"h\" + \">\")\n",
    "    \n",
    "    # Training the model with haikus similar to inputs\n",
    "    ngram_lm.train(similar_haikus)\n",
    "\n",
    "    haiku = ngram_lm.generate_haiku(1)\n",
    "    while \"<\" in haiku[0][0] or \"<\" in haiku[0][1] or \"<\" in haiku[0][2]:\n",
    "        haiku = ngram_lm.generate_haiku(1)\n",
    "        \n",
    "    return haiku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9efb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # make app\n",
    "    external_stylesheets = ['https://codepen.io/chriddyp/pen/bWLwgP.css']\n",
    "\n",
    "    # stylesheet with the .dbc class\n",
    "    dbc_css = \"https://cdn.jsdelivr.net/gh/AnnMarieW/dash-bootstrap-templates/dbc.min.css\"\n",
    "    app = JupyterDash(__name__, external_stylesheets=[dbc.themes.JOURNAL, dbc.themes.BOOTSTRAP, dbc_css])\n",
    "\n",
    "    channel_input = dcc.Input(\n",
    "        id=\"input-value\",\n",
    "        type=\"text\",\n",
    "        value=\"\",\n",
    "        size=\"lg\",\n",
    "        style={\"font-size\": \"1.6rem\", \"margin-top\": \".5px\"},\n",
    "        className=\"mb-3\"\n",
    "    )\n",
    "    button = dbc.Button(\n",
    "        id=\"search-button\",\n",
    "        children=\"Search\",\n",
    "        n_clicks=0,\n",
    "        size=\"lg\",\n",
    "        style={\"font-size\": \"1.2rem\", \"margin-left\": \"12px\", \"margin-top\": \"-8px\"},\n",
    "        color=\"primary\",\n",
    "        className=\"me-1\",\n",
    "    )\n",
    "\n",
    "    header = html.H1(\"NN and N-Gram Haiku Generator\",\n",
    "                     style={\"margin-top\": \"50px\"})\n",
    "\n",
    "    caption = html.H6(\"Generate Haikus by Topic\",\n",
    "                      style={\"margin-top\": \"10px\"})\n",
    "    \n",
    "    ngram = html.H6(\"N-Gram Generated Haiku\",\n",
    "                      style={\"margin-top\": \"10px\", \"font-weight\": \"bold\"})\n",
    "    \n",
    "    lstm = html.H6(\"LSTM Generated Haiku\",\n",
    "                      style={\"margin-top\": \"10px\", \"font-weight\": \"bold\"})\n",
    "\n",
    "    five1 = html.H6(id= \"firstline\",\n",
    "                    children=\"11111111\",\n",
    "                      style={\"margin-top\": \"10px\"})\n",
    "\n",
    "    seven1 = html.H6(id= \"secondline\",\n",
    "                    children=\"2222222222\",\n",
    "                      style={\"margin-top\": \"10px\"})\n",
    "\n",
    "    five2 = html.H6(id= \"thirdline\",\n",
    "                    children=\"33333333333\",\n",
    "                      style={\"margin-top\": \"10px\"})\n",
    "\n",
    "    five12 = html.H6(id= \"firstline2\",\n",
    "                    children=\"11111111\",\n",
    "                      style={\"margin-top\": \"10px\"})\n",
    "\n",
    "    seven12 = html.H6(id= \"secondline2\",\n",
    "                    children=\"2222222222\",\n",
    "                      style={\"margin-top\": \"10px\"})\n",
    "\n",
    "    five22 = html.H6(id= \"thirdline2\",\n",
    "                    children=\"33333333333\",\n",
    "                      style={\"margin-top\": \"10px\"})\n",
    "\n",
    "\n",
    "    img1 = html.Img(id= \"image1\", src=\"\")\n",
    "    img2 = html.Img(id= \"image2\", src=\"\")\n",
    "\n",
    "    collapse1 = html.Div(\n",
    "        [\n",
    "            dbc.Collapse(\n",
    "                dbc.Card(dbc.CardBody([ngram, html.Div(className='gap'),five1, html.Div(className='gap'), seven1, html.Div(className='gap'), five2\n",
    "                                       , html.Div(className='gap'), img1])),\n",
    "                id=\"collapse1\",\n",
    "                is_open=True,\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    collapse2 = html.Div(\n",
    "        [\n",
    "            dbc.Collapse(\n",
    "                dbc.Card(dbc.CardBody([lstm, html.Div(className='gap'),five12, html.Div(className='gap'), seven12, html.Div(className='gap'), five22\n",
    "                                       , html.Div(className='gap'), img2])),\n",
    "                id=\"collapse2\",\n",
    "                is_open=True,\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    app.layout = dbc.Container(\n",
    "        [\n",
    "\n",
    "        # top line of Dash\n",
    "        dbc.Row([\n",
    "            dbc.Col(\n",
    "                [header,caption, channel_input, button, collapse1, collapse2],\n",
    "                \n",
    "                lg=6\n",
    "            )\n",
    "        ],\n",
    "            justify = \"center\",\n",
    "            style = dict(textAlign=\"center\"),\n",
    "            className=\"d-flex justify-content-center\",\n",
    "        ),],\n",
    "        className=\"p-4\",\n",
    "        fluid = True)\n",
    "\n",
    "    @app.callback(\n",
    "        Output(\"search-button\", \"style\"),\n",
    "        Input(\"input-value\", \"value\"),\n",
    "    )\n",
    "    def change_button_color(channel_input):\n",
    "        if channel_input != \"\":\n",
    "            return {\"font-size\": \"1.2rem\", \"margin-left\": \"12px\", \"margin-top\": \"-8px\", \"background-color\": \"red\"}\n",
    "        else:\n",
    "            return {\"font-size\": \"1.2rem\", \"margin-left\": \"12px\", \"margin-top\": \"-8px\", 'background-color': 'gray'}\n",
    "\n",
    "    @app.callback(\n",
    "        Output(\"search-button\", \"n_clicks\"),\n",
    "        Output('firstline', 'children'),\n",
    "        Output('secondline', 'children'),\n",
    "        Output('thirdline', 'children'),\n",
    "        Output('image1', 'src'),\n",
    "        Output('firstline2', 'children'),\n",
    "        Output('secondline2', 'children'),\n",
    "        Output('thirdline2', 'children'),\n",
    "        Output('image2', 'src'),\n",
    "        Input(\"search-button\", \"n_clicks\"),\n",
    "        Input(\"input-value\", \"value\"),\n",
    "    )\n",
    "\n",
    "    def init_countdown_store(n_clicks, search_results):\n",
    "\n",
    "        lines = [\"\", \"\", \"\"]\n",
    "        imgsrc_ngram = \"\"\n",
    "        if n_clicks > 0:\n",
    "            lines = return_haikus(search_results)[0]\n",
    "            s = lines[0]+ lines[1]+ lines[2]\n",
    "            get_image(s, \"ngram\")\n",
    "            test_base64_ngram = base64.b64encode(open(\"image_ngram.png\", 'rb').read()).decode('ascii')\n",
    "            imgsrc_ngram = 'data:image/png;base64,{}'.format(test_base64_ngram)\n",
    "            \n",
    "        # NOTE - Code currently outputs haikus from the ngram instead of LSTM, can be easily implemented with the lstm\n",
    "        # haiku return fn\n",
    "        # The RNN does not currently produce effective or efficient haiku results\n",
    "        lines2 = [\"\", \"\", \"\"]\n",
    "        imgsrc_lstm = \"\"\n",
    "        if n_clicks > 0:\n",
    "            lines2 = return_haikus(search_results)[0]\n",
    "            s = lines2[0]+ lines2[1]+ lines2[2]\n",
    "            get_image(s, \"lstm\")\n",
    "            test_base64_lstm = base64.b64encode(open(\"image_lstm.png\", 'rb').read()).decode('ascii')\n",
    "            imgsrc_lstm = 'data:image/png;base64,{}'.format(test_base64_ngram)\n",
    "        \n",
    "        \n",
    "            # df['ID'] = df['ID'].str.slice(0, 3)\n",
    "        return 0, lines[0], lines[1], lines[2], imgsrc_ngram, lines2[0], lines2[1], lines2[2], imgsrc_lstm\n",
    "\n",
    "    app.run_server(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75186d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run server\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
